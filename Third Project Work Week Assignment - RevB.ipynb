{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Recommender Systems\n",
    "\n",
    "## Association Rule mining, Collaborative Filtering and Content Based Filtering\n",
    "\n",
    "\n",
    "### Brett Hallum, Mridul Jain, and Solomon Ndungu\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "The goal of this project is to analyze Movilens Dataset to understand. We will use this data to generate some of the movie recommendations for specific users, by looking at the movies they already watched and ratings they gave. By using the concepts of collaborative filtering we can find \" Movie \"X\" \"LIKED\" BY “SIMILAR” USERS as \"User-A\" \" and hence can be recommended to User-A as well.\n",
    "\n",
    "# Understanding the Data\n",
    "GroupLens Research has collected and made available rating data sets from the MovieLens web site (http://movielens.org). The data sets were collected over various periods of time, depending on the size of the set.\n",
    "There are multiple files in this dataset. There are 2 files that we are interested in u.data - this has the userId, the movieId, the rating and the date that rating was given. \n",
    "\n",
    "The data was collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from September 19th, 1997 through April 22nd, 1998. This data has been cleaned up - users who had less than 20 ratings or did not have complete demographic information were removed from this data set.\n",
    "\n",
    "u.data     -- The full u data set, 100000 ratings by 943 users on 1682 items.\n",
    "              Each user has rated at least 20 movies.  Users and items are\n",
    "              numbered consecutively from 1.  The data is randomly\n",
    "              ordered. This is a tab separated list of \n",
    "\t         user id | item id | rating | timestamp. \n",
    "              The time stamps are unix seconds since 1/1/1970 UTC   \n",
    "\n",
    "\n",
    "u.item     -- Information about the items (movies); this is a tab separated\n",
    "              list of\n",
    "              movie id | movie title | release date | video release date |\n",
    "              IMDb URL | unknown | Action | Adventure | Animation |\n",
    "              Children's | Comedy | Crime | Documentary | Drama | Fantasy |\n",
    "              Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |\n",
    "              Thriller | War | Western |\n",
    "              The last 19 fields are the genres, a 1 indicates the movie\n",
    "              is of that genre, a 0 indicates it is not; movies can be in\n",
    "              several genres at once.\n",
    "              The movie ids are the ones used in the u.data data set.\n",
    "\n",
    "\n",
    "# Data Exploration and Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\emrijai\\\\Documents\\\\IPython Notebooks\\\\MS7331\\\\Project3\\\\ml-100k\\\\ml-100k'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/emrijai/Documents/IPython Notebooks/MS7331/Project3/ml-100k/ml-100k/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Files to be used for analysis\n",
    "\n",
    "dataFile='u.data'\n",
    "movieInfoFile='u.item'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We are passing the header explicitly as there is no header info in the files\n",
    "#We are not interested in all the columns of 'u.item'. We are going to use only 0,1 columns from this file.\n",
    "\n",
    "data=pd.read_csv(dataFile,sep=\"\\t\",header=None,names=['userId','itemId','rating','timestamp'])\n",
    "movieInfo=pd.read_csv(movieInfoFile,sep=\"|\", header=None, index_col=False,\n",
    "                     names=['itemId','title'], usecols=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  itemId  rating  timestamp\n",
      "0     196     242       3  881250949\n",
      "1     186     302       3  891717742\n",
      "2      22     377       1  878887116\n",
      "3     244      51       2  880606923\n",
      "4     166     346       1  886397596\n",
      "\n",
      "\n",
      "   itemId              title\n",
      "0       1   Toy Story (1995)\n",
      "1       2   GoldenEye (1995)\n",
      "2       3  Four Rooms (1995)\n",
      "3       4  Get Shorty (1995)\n",
      "4       5     Copycat (1995)\n"
     ]
    }
   ],
   "source": [
    "print data.head()\n",
    "print '\\n'\n",
    "print movieInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  itemId  rating  timestamp         title\n",
      "0     196     242       3  881250949  Kolya (1996)\n",
      "1      63     242       3  875747190  Kolya (1996)\n",
      "2     226     242       5  883888671  Kolya (1996)\n",
      "3     154     242       3  879138235  Kolya (1996)\n",
      "4     306     242       5  876503793  Kolya (1996)\n"
     ]
    }
   ],
   "source": [
    "# Merging the two files together into one single dataFrame. We will use this dataFrame in the further analysis.\n",
    "\n",
    "data=pd.merge(data,movieInfo,left_on='itemId',right_on=\"itemId\")\n",
    "\n",
    "# Create a combined csv file that we will use to load in pyspark for Latent Factor Collaborative filtering \\\n",
    "# using ALTERNATING LEAST SQUARES method\n",
    "data.to_csv('combined_user_movie_file.csv')\n",
    "print (data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f84815cea620>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgraphlab\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgl_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgl_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\graphlab\\__init__.pyc\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mgraphlab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoolkits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_engineering\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeature_engineering\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mgraphlab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoolkits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_parameter_search\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmodel_parameter_search\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mgraphlab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoolkits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mgraphlab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoolkits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_parameter_search\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\graphlab\\toolkits\\model_parameter_search\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'grid_search'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'random_search'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'manual_search'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmanual_search\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\graphlab\\toolkits\\model_parameter_search\\manual_search.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_model_parameter_search_evaluators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefault_evaluator\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_default_evaluator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_model_parameter_search\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_create_model_search\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_model_parameter_search\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_add_docstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_DATA_DOCSTRING\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_MODEL_FACTORY_DOCSTRING\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_MODEL_PARAMETERS_DOCSTRING\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_EVALUATOR_DOCSTRING\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ENVIRONMENT_DOCSTRING\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_RETURN_MODEL_DOCSTRING\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_RETURNS_DOCSTRING\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_PERFORM_TRIAL_RUN_DOCSTRING\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m @_add_docstring(param_data=_DATA_DOCSTRING,\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\graphlab\\toolkits\\model_parameter_search\\_model_parameter_search.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgraphlab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSFrame\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_SFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgraphlab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSArray\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_SArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgraphlab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeploy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmap_job\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_map_job\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgraphlab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_get_metric_tracker\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_model_parameter_search_evaluators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefault_evaluator\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_default_evaluator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\graphlab\\deploy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Sessions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_session\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0m_default_session\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgraphlab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_is_string\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\graphlab\\deploy\\_session.pyc\u001b[0m in \u001b[0;36m_open\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[0m__LOGGER__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using session dir: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\graphlab\\deploy\\_session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, location)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mversion_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'version'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mversion_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[0mversion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_SESSION_VERSION\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\graphlab\\util\\lockfile\\__init__.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mContext\u001b[0m \u001b[0mmanager\u001b[0m \u001b[0msupport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \"\"\"\n\u001b[1;32m--> 238\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\graphlab\\util\\lockfile\\mkdirlockfile.pyc\u001b[0m in \u001b[0;36macquire\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m     54\u001b[0m                             raise AlreadyLocked(\"%s is already locked\" %\n\u001b[0;32m     55\u001b[0m                                                 self.path)\n\u001b[1;32m---> 56\u001b[1;33m                     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[1;31m# Couldn't create the lock for some other reason\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import graphlab as gl\n",
    "\n",
    "gl_data = gl.SFrame(data)\n",
    "print (gl_data.head())\n",
    "\n",
    "model = gl.recommender.create(gl_data, user_id=\"userId\", item_id=\"title\", target=\"rating\")\n",
    "results = model.recommend(users=None, k=5)\n",
    "model.save(\"my_model\")\n",
    "\n",
    "results.head() # the recommendation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "item_item = gl.recommender.item_similarity_recommender.create(gl_data, \n",
    "                                  user_id=\"userId\", \n",
    "                                  item_id=\"title\", \n",
    "                                  target=\"rating\",\n",
    "                                  only_top_k=5,\n",
    "                                  similarity_type=\"cosine\")\n",
    "\n",
    "results = item_item.get_similar_items(k=5)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = gl.recommender.util.random_split_by_user(gl_data,\n",
    "                                                    user_id=\"userId\", item_id=\"title\",\n",
    "                                                    max_num_users=100, item_test_proportion=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "\n",
    "gl.canvas.set_target('ipynb')\n",
    "\n",
    "\n",
    "item_item = gl.recommender.item_similarity_recommender.create(train, \n",
    "                                  user_id=\"userId\", \n",
    "                                  item_id=\"title\", \n",
    "                                  target=\"rating\",\n",
    "                                  only_top_k=5,\n",
    "                                  similarity_type=\"cosine\")\n",
    "\n",
    "rmse_results = item_item.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print rmse_results.viewkeys()\n",
    "print rmse_results['rmse_by_item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmse_results['rmse_by_user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmse_results['precision_recall_by_user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab.aggregate as agg\n",
    "\n",
    "# we will be using these aggregations\n",
    "agg_list = [agg.AVG('precision'),agg.STD('precision'),agg.AVG('recall'),agg.STD('recall')]\n",
    "\n",
    "# apply these functions to each group (we will group the results by 'k' which is the cutoff)\n",
    "# the cutoff is the number of top items to look for see the following URL for the actual equation\n",
    "# https://dato.com/products/create/docs/generated/graphlab.recommender.util.precision_recall_by_user.html#graphlab.recommender.util.precision_recall_by_user\n",
    "rmse_results['precision_recall_by_user'].groupby('cutoff',agg_list)\n",
    "\n",
    "# the groups are not sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validated Collab Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rec1 = gl.recommender.ranking_factorization_recommender.create(train, \n",
    "                                  user_id=\"userId\", \n",
    "                                  item_id=\"title\", \n",
    "                                  target=\"rating\")\n",
    "\n",
    "rmse_results = rec1.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmse_results['precision_recall_by_user'].groupby('cutoff',[agg.AVG('precision'),agg.STD('precision'),agg.AVG('recall'),agg.STD('recall')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rec1 = gl.recommender.ranking_factorization_recommender.create(train, \n",
    "                                  user_id=\"userId\", \n",
    "                                  item_id=\"title\", \n",
    "                                  target=\"rating\",\n",
    "                                  num_factors=16,                 # override the default value\n",
    "                                  regularization=1e-02,           # override the default value\n",
    "                                  linear_regularization = 1e-3)   # override the default value\n",
    "\n",
    "rmse_results = rec1.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to Item-Item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comparison = gl.recommender.util.compare_models(test, [item_item, rec1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " comparisonstruct = gl.compare(test,[item_item, rec1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gl.show_comparison(comparisonstruct,[item_item, rec1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'user_id': 'userId', \n",
    "          'item_id': 'title', \n",
    "          'target': 'rating',\n",
    "          'num_factors': [8, 12, 16, 24, 32], \n",
    "          'regularization':[0.001] ,\n",
    "          'linear_regularization': [0.001]}\n",
    "\n",
    "job = gl.model_parameter_search.create( (train,test),\n",
    "        gl.recommender.ranking_factorization_recommender.create,\n",
    "        params,\n",
    "        max_models=5,\n",
    "        environment=None)\n",
    "\n",
    "# also note thatthis evaluator also supports sklearn\n",
    "# https://dato.com/products/create/docs/generated/graphlab.toolkits.model_parameter_search.create.html?highlight=model_parameter_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bst_prms = job.get_best_params()\n",
    "bst_prms\n",
    "models = job.get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comparisonstruct = gl.compare(test,models)\n",
    "gl.show_comparison(comparisonstruct,models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comparisonstruct = gl.compare(test,[models[4], item_item])\n",
    "gl.show_comparison(comparisonstruct,[models[4], item_item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 5)\n",
      "   userId  itemId  rating  timestamp         title\n",
      "0     196     242       3  881250949  Kolya (1996)\n",
      "1      63     242       3  875747190  Kolya (1996)\n",
      "2     226     242       5  883888671  Kolya (1996)\n",
      "3     154     242       3  879138235  Kolya (1996)\n",
      "4     306     242       5  876503793  Kolya (1996)\n"
     ]
    }
   ],
   "source": [
    "print data.shape\n",
    "print data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users:  943\n",
      "Number of Movies:  1682\n",
      "\n",
      "\n",
      "Number of users that rate a particular Movie: \n",
      "\n",
      "Star Wars (1977)             583\n",
      "Contact (1997)               509\n",
      "Fargo (1996)                 508\n",
      "Return of the Jedi (1983)    507\n",
      "Liar Liar (1997)             485\n",
      "Name: title, dtype: int64\n",
      "\n",
      "\n",
      "Number of movies rated by particular User: \n",
      "\n",
      "405    737\n",
      "655    685\n",
      "13     636\n",
      "450    540\n",
      "276    518\n",
      "Name: userId, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data=pd.DataFrame.sort_values(data,['userId','itemId'],ascending=[0,1])\n",
    "\n",
    "# Let's see how many users and how  many movies there are \n",
    "numUsers=max(data.userId)\n",
    "numMovies=max(data.itemId)\n",
    "\n",
    "moviesPerUser=data.userId.value_counts()\n",
    "usersPerMovie=data.title.value_counts()\n",
    "\n",
    "print 'Number of Users: ', numUsers\n",
    "print 'Number of Movies: ', numMovies\n",
    "print '\\n'\n",
    "print 'Number of users that rate a particular Movie: \\n\\n', usersPerMovie.head()\n",
    "print '\\n'\n",
    "print 'Number of movies rated by particular User: \\n\\n', moviesPerUser.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>itemId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23781</th>\n",
       "      <td>943</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>888639953</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65410</th>\n",
       "      <td>943</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>875501960</td>\n",
       "      <td>Dead Man Walking (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35098</th>\n",
       "      <td>943</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>888639000</td>\n",
       "      <td>Seven (Se7en) (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43773</th>\n",
       "      <td>943</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>888639093</td>\n",
       "      <td>Usual Suspects, The (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57040</th>\n",
       "      <td>943</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>888639042</td>\n",
       "      <td>Braveheart (1995)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  itemId  rating  timestamp                       title\n",
       "23781     943       2       5  888639953            GoldenEye (1995)\n",
       "65410     943       9       3  875501960     Dead Man Walking (1995)\n",
       "35098     943      11       4  888639000        Seven (Se7en) (1995)\n",
       "43773     943      12       5  888639093  Usual Suspects, The (1995)\n",
       "57040     943      22       4  888639042           Braveheart (1995)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to return the topN Movies for a specific user. N is an arbitrary number, and can be changed as needed.\n",
    "\n",
    "def topN(activeUser,N):\n",
    "    user_topN = data.loc[data.userId == activeUser]\n",
    "    return user_topN.loc[user_topN.rating > 4].head(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([405, 655, 13, 450, 276, 416, 537, 303, 234, 393], dtype='int64')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moviesPerUser.index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Movies that are rated highly by most active movie raters in the dataset\n",
      "\n",
      "Star Wars (1977)                                                               15\n",
      "Godfather, The (1972)                                                          13\n",
      "Usual Suspects, The (1995)                                                     11\n",
      "Monty Python and the Holy Grail (1974)                                         10\n",
      "Pulp Fiction (1994)                                                            10\n",
      "Apocalypse Now (1979)                                                           9\n",
      "Jaws (1975)                                                                     9\n",
      "Schindler's List (1993)                                                         9\n",
      "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)     9\n",
      "Empire Strikes Back, The (1980)                                                 9\n",
      "Name: title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "TopMoviesList = pd.DataFrame()\n",
    "\n",
    "Num_Active_Critics_to_Check = 20\n",
    "Num_Movies_by_Each_Critic = 500\n",
    "\n",
    "for i in moviesPerUser.index[:Num_Active_Critics_to_Check]:\n",
    "    TopMoviesList = TopMoviesList.append(topN(i,Num_Movies_by_Each_Critic))\n",
    "\n",
    "del TopMoviesList['userId']\n",
    "del TopMoviesList['timestamp']\n",
    "\n",
    "#Atleast 20% of the critics are agreein to the top rating for the movies\n",
    "\n",
    "TopMoviesList = TopMoviesList.title.value_counts()\n",
    "TopMoviesList = TopMoviesList[TopMoviesList>Num_Active_Critics_to_Check/5]\n",
    "\n",
    "print '\\nMovies that are rated highly by most active movie raters in the dataset\\n\\n', TopMoviesList.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Since userID 405 is the most active user and seems like a movie buff. Its a good idea to check which movies he liked\n",
    "# Lets see user ID 405's highest and lowest rated movies.\n",
    "\n",
    "user_405 = data.loc[data.userId == 405]\n",
    "user_405_HighestRatings = user_405.loc[user_405.rating > 4]\n",
    "user_405_LowestRatings = user_405.loc[user_405.rating < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Highest Rated Movies by UserID 405        userId  itemId  rating  timestamp                       title\n",
      "43709     405      12       5  885545306  Usual Suspects, The (1995)\n",
      "56861     405      22       5  885545167           Braveheart (1995)\n",
      "14992     405      23       5  885545372          Taxi Driver (1976)\n",
      "68788     405      38       5  885548093             Net, The (1995)\n",
      "48303     405      47       5  885545429              Ed Wood (1994)\n",
      "\n",
      "5 Lowest Rated Movies by UserID 405        userId  itemId  rating  timestamp                 title\n",
      "23701     405       2       1  885547953      GoldenEye (1995)\n",
      "72281     405      27       1  885546487       Bad Boys (1995)\n",
      "89654     405      30       1  885549544  Belle de jour (1967)\n",
      "87587     405      31       1  885548579   Crimson Tide (1995)\n",
      "6166      405      32       1  885546025          Crumb (1994)\n"
     ]
    }
   ],
   "source": [
    "print '5 Highest Rated Movies by UserID 405', user_405_HighestRatings.head(5)\n",
    "print '\\n5 Lowest Rated Movies by UserID 405', user_405_LowestRatings.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the personalized recommendation scenario, the introduction of new users or new items can \n",
    "cause the cold start problem, as there will be insufficient data on these new entries for the \n",
    "collaborative filtering to work accurately\n",
    "Next we can quickly find the active raters, we call them Movie Critics, and see which movies they rated highest\n",
    "and which movies they rated lowest. These movies in general can be recommended to the people who have not rated\n",
    "or seen any movies yet, and are new to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to return the topN Movies for a specific user. N is an arbitrary number, and can be changed as needed.\n",
    "\n",
    "def bottomN(activeUser,N):\n",
    "    user_bottomN = data.loc[data.userId == activeUser]\n",
    "    return user_bottomN.loc[user_bottomN.rating < 3].head(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Movies that are rated low by most active movie raters in the dataset\n",
      "\n",
      "Batman Forever (1995)                8\n",
      "Very Brady Sequel, A (1996)          7\n",
      "Volcano (1997)                       7\n",
      "Waterworld (1995)                    7\n",
      "Die Hard: With a Vengeance (1995)    7\n",
      "Natural Born Killers (1994)          7\n",
      "Pretty Woman (1990)                  7\n",
      "Lord of Illusions (1995)             6\n",
      "Free Willy (1993)                    6\n",
      "Long Kiss Goodnight, The (1996)      6\n",
      "Name: title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "bottomMoviesList = pd.DataFrame()\n",
    "\n",
    "Num_Active_Critics_to_Check = 20\n",
    "Num_Movies_by_Each_Critic = 500\n",
    "\n",
    "for i in moviesPerUser.index[:Num_Active_Critics_to_Check]:\n",
    "    bottomMoviesList = bottomMoviesList.append(bottomN(i,Num_Movies_by_Each_Critic))\n",
    "\n",
    "del bottomMoviesList['userId']\n",
    "del bottomMoviesList['timestamp']\n",
    "\n",
    "#Atleast 20% of the critics are agreein to the bottom rating for the movies\n",
    "\n",
    "bottomMoviesList = bottomMoviesList.title.value_counts()\n",
    "bottomMoviesList = bottomMoviesList[bottomMoviesList>Num_Active_Critics_to_Check/5]\n",
    "\n",
    "print '\\nMovies that are rated low by most active movie raters in the dataset\\n\\n', bottomMoviesList.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import correlation \n",
    "def similarity(user1,user2):\n",
    "    user1=np.array(user1)-np.nanmean(user1) # we are first normalizing user1 by \n",
    "    # the mean rating of user 1 for any movie. Note the use of np.nanmean() - this \n",
    "    # returns the mean of an array after ignoring and NaN values \n",
    "    user2=np.array(user2)-np.nanmean(user2)\n",
    "    # Now to find the similarity between 2 users\n",
    "    # We'll first subset each user to be represented only by the ratings for the \n",
    "    # movies the 2 users have in common \n",
    "    commonItemIds=[i for i in range(len(user1)) if user1[i]>0 and user2[i]>0]\n",
    "    # Gives us movies for which both users have non NaN ratings \n",
    "    if len(commonItemIds)==0:\n",
    "        # If there are no movies in common \n",
    "        return 0\n",
    "    else:\n",
    "        user1=np.array([user1[i] for i in commonItemIds])\n",
    "        user2=np.array([user2[i] for i in commonItemIds])\n",
    "        return correlation(user1,user2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Men in Black (1997)', 'Blade Runner (1982)', 'Empire Strikes Back, The (1980)']\n"
     ]
    }
   ],
   "source": [
    "# Let's write a function to find the top N favorite movies of a user \n",
    "def favoriteMovies(activeUser,N):\n",
    "    #1. subset the dataframe to have the rows corresponding to the active user\n",
    "    # 2. sort by the rating in descending order\n",
    "    # 3. pick the top N rows\n",
    "    topMovies=pd.DataFrame.sort_values(\n",
    "        data[data.userId==activeUser],['rating'],ascending=[0])[:N]\n",
    "    # return the title corresponding to the movies in topMovies \n",
    "    return list(topMovies.title)\n",
    "\n",
    "print favoriteMovies(5,3) # Print the top 3 favorite movies of user 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a very sparse Matrix \"user_to_Movie_Rating_Matrix\" of UserID and MovieRatig which we will use later \n",
    "# on to find the user-user correlation and hence will be able to find which users are similar to each other.\n",
    "\n",
    "userItemRatingMatrix=pd.pivot_table(data, values='rating',\n",
    "                                    index=['userId'], columns=['itemId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>itemId</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>1673</th>\n",
       "      <th>1674</th>\n",
       "      <th>1675</th>\n",
       "      <th>1676</th>\n",
       "      <th>1677</th>\n",
       "      <th>1678</th>\n",
       "      <th>1679</th>\n",
       "      <th>1680</th>\n",
       "      <th>1681</th>\n",
       "      <th>1682</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1682 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "itemId  1     2     3     4     5     6     7     8     9     10    ...   \\\n",
       "userId                                                              ...    \n",
       "1        5.0   3.0   4.0   3.0   3.0   5.0   4.0   1.0   5.0   3.0  ...    \n",
       "2        4.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   2.0  ...    \n",
       "3        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    \n",
       "4        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    \n",
       "5        4.0   3.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...    \n",
       "\n",
       "itemId  1673  1674  1675  1676  1677  1678  1679  1680  1681  1682  \n",
       "userId                                                              \n",
       "1        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "2        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "3        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "4        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "5        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[5 rows x 1682 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userItemRatingMatrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering\n",
    "\n",
    "#### Memory-based: Find similar users (user-based CF) or items (item-based CF) to predict missing ratings\n",
    "1. Produce recommendations based on the preferences of similar users \n",
    "\t(Goldberg et al., 1992; Resnick et al., 1994; Mild and Reutterer, 2001)\n",
    "2. Produce recommendations based on the relationship between items in the user-item matrix \n",
    "\t(Kitts et al., 2000; Sarwar et al., 2001)\n",
    "\n",
    "#### Model-based: Build a model from the rating data (clustering, latent semantic structure, etc.) and then use this model to predict missing ratings\n",
    "\n",
    "There are many techniques:\n",
    "\n",
    "1. Cluster users and then recommend items the users in the cluster closest to the active user like\n",
    "2. Mine association rules and then use the rules to recommend items (for binary/binarized data)\n",
    "3. Define a null-model (a stochastic process which models usage of independent items) and then find significant deviation from the null-model\n",
    "4. Learn a latent factor model from the data and then use the discovered factors to find items with high expected ratings\n",
    "\n",
    "First we are going to use the K Nearest Neighbors technique (Memory Based Collaborative Filtering technique)\n",
    "To achieve this we are going to create a K-Nearest Neighbors (Similar Users) of the user in question, and looking at \"Neighbors / Similar Users\" ratings for a specific item/movie, predict the rating for the user in question.\n",
    "\n",
    "The idea here is to predict users ratings for the Movies/Products they have not yet rated based on the ratings or feedback received by other users who are in one way or other very similar to the user we are trying to recommend/predict for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nearestNeighbourRatings(activeUser,K):\n",
    "    # This function will find the K Nearest neighbours of the active user, then \n",
    "    # use their ratings to predict the activeUsers ratings for other movies \n",
    "    similarityMatrix=pd.DataFrame(index=userItemRatingMatrix.index,\n",
    "                                  columns=['Similarity'])\n",
    "    # Creates an empty matrix whose row index is userIds, and the value will be \n",
    "    # similarity of that user to the active User\n",
    "    for i in userItemRatingMatrix.index:\n",
    "        similarityMatrix.loc[i]=similarity(userItemRatingMatrix.loc[activeUser],\n",
    "                                          userItemRatingMatrix.loc[i])\n",
    "        # Find the similarity between user i and the active user and add it to the \n",
    "        # similarityMatrix \n",
    "    similarityMatrix=pd.DataFrame.sort_values(similarityMatrix,\n",
    "                                              ['Similarity'],ascending=[0])\n",
    "    # Sort the similarity matrix in the descending order of similarity \n",
    "    nearestNeighbours=similarityMatrix[:K]\n",
    "    # The above line will give us the K Nearest neighbours \n",
    "    \n",
    "    # We'll now take the nearest neighbours and use their ratings \n",
    "    # to predict the active user's rating for every movie\n",
    "    neighbourItemRatings=userItemRatingMatrix.loc[nearestNeighbours.index]\n",
    "    # The similarity matrix had an index which was the userId, By sorting \n",
    "    # and picking the top K rows, the nearestNeighbours dataframe now has \n",
    "    # a dataframe whose row index is the userIds of the K Nearest neighbours \n",
    "    # Using this index we can directly find the corresponding rows in the \n",
    "    # user Item rating matrix \n",
    "    predictItemRating=pd.DataFrame(index=userItemRatingMatrix.columns, columns=['Rating'])\n",
    "    # A placeholder for the predicted item ratings. It's row index is the \n",
    "    # list of itemIds which is the same as the column index of userItemRatingMatrix\n",
    "    #Let's fill this up now\n",
    "    for i in userItemRatingMatrix.columns:\n",
    "        # for each item \n",
    "        predictedRating=np.nanmean(userItemRatingMatrix.loc[activeUser])\n",
    "        # start with the average rating of the user\n",
    "        for j in neighbourItemRatings.index:\n",
    "            # for each neighbour in the neighbour list \n",
    "            if userItemRatingMatrix.loc[j,i]>0:\n",
    "                # If the neighbour has rated that item\n",
    "                # Add the rating of the neighbour for that item\n",
    "                #    adjusted by \n",
    "                #    the average rating of the neighbour \n",
    "                #    weighted by \n",
    "                #    the similarity of the neighbour to the active user\n",
    "                predictedRating += (userItemRatingMatrix.loc[j,i]\n",
    "                                    -np.nanmean(userItemRatingMatrix.loc[j]))*nearestNeighbours.loc[j,'Similarity']\n",
    "        # We are out of the loop which uses the nearest neighbours, add the \n",
    "        # rating to the predicted Rating matrix\n",
    "        predictItemRating.loc[i,'Rating']=predictedRating\n",
    "    return predictItemRating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's now use these predicted Ratings to find the top N Recommendations for the active user \n",
    "\n",
    "def topNRecommendations(activeUser,N):\n",
    "    predictItemRating=nearestNeighbourRatings(activeUser,10)\n",
    "    # Use the 10 nearest neighbours to find the predicted ratings\n",
    "    moviesAlreadyWatched=list(userItemRatingMatrix.loc[activeUser]\n",
    "                              .loc[userItemRatingMatrix.loc[activeUser]>0].index)\n",
    "    # find the list of items whose ratings which are not NaN\n",
    "    predictItemRating=predictItemRating.drop(moviesAlreadyWatched)\n",
    "    topRecommendations=pd.DataFrame.sort_values(predictItemRating,\n",
    "                                                ['Rating'],ascending=[0])[:N]\n",
    "    # This will give us the list of itemIds which are the top recommendations \n",
    "    # Let's find the corresponding movie titles \n",
    "    topRecommendationTitles=(movieInfo.loc[movieInfo.itemId.isin(topRecommendations.index)])\n",
    "    return list(topRecommendationTitles.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Men in Black (1997)', 'Blade Runner (1982)', 'Empire Strikes Back, The (1980)', 'Wrong Trousers, The (1993)', 'Blues Brothers, The (1980)'] \n",
      "['Truth About Cats & Dogs, The (1996)', 'Scream (1996)', 'First Wives Club, The (1996)']\n"
     ]
    }
   ],
   "source": [
    "# Let's use this for one specific user and predict the top N recommendations for that user\n",
    "activeUser=5\n",
    "print favoriteMovies(activeUser,5),\"\\n\",topNRecommendations(activeUser,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LATENT FACTOR COLLABORATIVE FILTERING\n",
    "\n",
    "The objective of Matrix Factoriation is to decompose each user rating into a user-factor vector and a product-factor vector. This is analogous to what happens in singular value decomposition or principal component analysis. However, these techniques would only make sense if you knew all the ratings for all the users for all products, which is not the case in the case of user-movie rating.\n",
    "In order to overcome this issue, we only solve for the ratings which are available.\n",
    "\n",
    "Next we are going to use model based approach by using Latent Factor and Association Rules mining to predict the ratings and recommend the movies to users.\n",
    "\n",
    "#### Two popular methods to solve matrix factorization for recommendations\n",
    "\n",
    "1. STOCHASTIC GRADIENT DESCENT\n",
    "2. ALTERNATING LEAST SQUARES\n",
    "\n",
    "### We will implement SGD algorithm manually and ALS using SPARK's MLLib Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's now use matrix factorization to do the same exercise ie\n",
    "# finding the recommendations for a user\n",
    "# The idea here is to identify some factors (these are factors which influence\n",
    "# a user'r rating). The factors are identified by decomposing the \n",
    "# user item rating matrix into a user-factor matrix and a item-factor matrix\n",
    "# Each row in the user-factor matrix maps the user onto the hidden factors\n",
    "# Each row in the product factor matrix maps the item onto the hidden factors\n",
    "# This operation will be pretty expensive because it will effectively give us \n",
    "# the factor vectors needed to find the rating of any product by any user \n",
    "# (in the  previous case (KNN) we only did the computations for 1 user)\n",
    "\n",
    "def matrixFactorization(R, K, steps=10, gamma=0.001,lamda=0.02):\n",
    "    # R is the user item rating matrix \n",
    "    # K is the number of factors we will find \n",
    "    # We'll be using Stochastic Gradient descent to find the factor vectors \n",
    "    # steps, gamma and lamda are parameters the SGD will use - we'll get to them\n",
    "    # in a bit \n",
    "    N=len(R.index)# Number of users\n",
    "    M=len(R.columns) # Number of items \n",
    "    P=pd.DataFrame(np.random.rand(N,K),index=R.index)\n",
    "    # This is the user factor matrix we want to find. It will have N rows \n",
    "    # on for each user and K columns, one for each factor. We are initializing \n",
    "    # this matrix with some random numbers, then we will iteratively move towards \n",
    "    # the actual value we want to find \n",
    "    Q=pd.DataFrame(np.random.rand(M,K),index=R.columns)\n",
    "    # This is the product factor matrix we want to find. It will have M rows, \n",
    "    # one for each product/item/movie. \n",
    "    for step in xrange(steps):\n",
    "        # SGD will loop through the ratings in the user item rating matrix \n",
    "        # It will do this as many times as we specify (number of steps) or \n",
    "        # until the error we are minimizing reaches a certain threshold \n",
    "        for i in R.index:\n",
    "            for j in R.columns:\n",
    "                if R.loc[i,j]>0:\n",
    "                    # For each rating that exists in the training set \n",
    "                    eij=R.loc[i,j]-np.dot(P.loc[i],Q.loc[j])\n",
    "                    # This is the error for one rating \n",
    "                    # ie difference between the actual value of the rating \n",
    "                    # and the predicted value (dot product of the corresponding \n",
    "                    # user factor vector and item-factor vector)\n",
    "                    # We have an error function to minimize. \n",
    "                    # The Ps and Qs should be moved in the downward direction \n",
    "                    # of the slope of the error at the current point \n",
    "                    P.loc[i]=P.loc[i]+gamma*(eij*Q.loc[j]-lamda*P.loc[i])\n",
    "                    # Gamma is the size of the step we are taking / moving the value\n",
    "                    # of P by \n",
    "                    # The value in the brackets is the partial derivative of the \n",
    "                    # error function ie the slope. Lamda is the value of the \n",
    "                    # regularization parameter which penalizes the model for the \n",
    "                    # number of factors we are finding. \n",
    "                    Q.loc[j]=Q.loc[j]+gamma*(eij*P.loc[i]-lamda*Q.loc[j])\n",
    "        # At the end of this we have looped through all the ratings once. \n",
    "        # Let's check the value of the error function to see if we have reached \n",
    "        # the threshold at which we want to stop, else we will repeat the process\n",
    "        e=0\n",
    "        for i in R.index:\n",
    "            for j in R.columns:\n",
    "                if R.loc[i,j]>0:\n",
    "                    #Sum of squares of the errors in the rating\n",
    "                    e= e + pow(R.loc[i,j]-np.dot(P.loc[i],Q.loc[j]),2)+lamda*(pow(np.linalg.norm(P.loc[i]),2)+pow(np.linalg.norm(Q.loc[j]),2))\n",
    "        if e<0.001:\n",
    "            break\n",
    "        print step\n",
    "    return P,Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Let's call this function now \n",
    "(P,Q)=matrixFactorization(userItemRatingMatrix.iloc[:100,:100],K=2,gamma=0.001,lamda=0.02, steps=25)\n",
    "# Ideally we should run this over the entire matrix for a few 1000's steps, \n",
    "# This will be pretty expensive computationally. For now lets just do it over a \n",
    "# part of the rating matrix to see how it works. We've kept the steps to 25. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Star Wars (1977)', 'Good Will Hunting (1997)', 'L.A. Confidential (1997)', 'Titanic (1997)', \"Schindler's List (1993)\"]\n"
     ]
    }
   ],
   "source": [
    "# Let's quickly use these ratings to find top recommendations for a user \n",
    "activeUser=5\n",
    "predictItemRating=pd.DataFrame(np.dot(P.loc[activeUser],Q.T),index=Q.index,columns=['Rating'])\n",
    "topRecommendations=pd.DataFrame.sort_values(predictItemRating,['Rating'],ascending=[0])[:5]\n",
    "# We found the ratings of all movies by the active user and then sorted them to find the top 3 movies \n",
    "topRecommendationTitles=movieInfo.loc[movieInfo.itemId.isin(topRecommendations.index)]\n",
    "print list(topRecommendationTitles.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### SPARK MLLib for Latent Factor Collaborative Filtering - Matrix Factorization\n",
    "#### Alternative Least Squares Method for Calculating "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To run spark on local laptop/machine we install Spark from http://spark.apache.org/downloads.html, and follow the steps. \n",
    "\n",
    "1.Choose a Spark release: 2.0.0 (Jul 26 2016)1.6.2 \n",
    "2.Choose a package type: Pre-built for Hadoop 2.6  \n",
    "3.Choose a download type: Direct DownloadSelect Apache Mirror\n",
    "4.Download Spark: spark-2.0.0-bin-hadoop2.6.tgz\n",
    "\n",
    "Once downloaded, unzip the binaries in a specific folder like \"C:\\Apache-Spark\\spark-2.0.0-bin-hadoop2.6\" and change directory to this folder.\n",
    "From shell/command prompt, run ./bin/spark-shell\n",
    "This will start Spark Shell on the local machine.\n",
    "\n",
    "Set the path variable or .bash_profile as needed: \n",
    "In case of Windows PATH: C:\\Apache-Spark\\spark-2.0.0-bin-hadoop2.6;C:\\Apache-Spark\\spark-2.0.0-bin-hadoop2.6\\bin;\n",
    "\n",
    "Next we will configure pyspark context in IPYTHON Notebook, which will enable us to use all the features of SPARK right from IPYTHON Notebook. pyspark is a python shell with all the fucntionalities and libraries from SPARK like MLLib etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.0.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.11 (default, Feb 16 2016 09:58:36)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# Below code will enable Spark Shell from IPYTHON\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError ('SPARK_HOME environment variable not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python\\lib\\pyspark.zip')) \n",
    "sys.path.insert(0, os.path.join(spark_home, 'python\\lib\\py4j-0.9-src.zip')) \n",
    "\n",
    "execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u',userId,itemId,rating,timestamp,title',\n",
       " u'0,196,242,3,881250949,Kolya (1996)',\n",
       " u'1,63,242,3,875747190,Kolya (1996)',\n",
       " u'2,226,242,5,883888671,Kolya (1996)',\n",
       " u'3,154,242,3,879138235,Kolya (1996)',\n",
       " u'4,306,242,5,876503793,Kolya (1996)',\n",
       " u'5,296,242,4,884196057,Kolya (1996)',\n",
       " u'6,34,242,5,888601628,Kolya (1996)',\n",
       " u'7,271,242,4,885844495,Kolya (1996)',\n",
       " u'8,201,242,4,884110598,Kolya (1996)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uadatapath=\"C:/Users/emrijai/Documents/IPython Notebooks/MS7331/Project3/ml-100k/ml-100k/combined_user_movie_file.csv\"\n",
    "rawUserArtistData = sc.textFile(uadatapath)\n",
    "rawUserArtistData.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter the header out \n",
    "rawUserMovieData_wo_header = rawUserMovieData.filter(lambda x:\"userId\" not in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawUserMovieData_wo_header.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawUserMovieData_wo_header.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the ratings column nwhere ratings is 4 or 5\n",
    "# The code below gives the mean rating given to the movies (average of all the ratings) in the dataset\n",
    "rawUserMovieData_wo_header.map(lambda x:float(x.split(\",\")[3])).stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import Rating,ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the ratings column nwhere ratings is 4 or 5\n",
    "# Since we are running this algorithm on a local machine, filtering low ratings will help\n",
    "# 1. Reduce the amount of processing\n",
    "# 2. Reduce the amount of data held in-memory\n",
    "\n",
    "# Since \"rawUserMovieData_wo_header\" is an RDD of Strings, we need to convert this into RDD of Rating objects\n",
    "# Additionally, we have filtered out any ratings that are below 4\n",
    "# Convert the list into a Rating object (Line #4 below)\n",
    "# Using persist function, ALS will pass over this RDD many times. Persisting will make the computation much faster\n",
    "\n",
    "uaData=rawUserMovieData_wo_header\\\n",
    "    .map(lambda x:x.split(\",\"))\\\n",
    "    .filter(lambda x: float(x[3])>=4)\\\n",
    "    .map(lambda x:Rating(x[1],x[2],x[3]))\n",
    "uaData.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uaData.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ALS has 2 methods : train and trainImplicit. Since our ratings are explicit we use the train method.\n",
    "# Explicit vs. implicit feedback\n",
    "# The standard approach to matrix factorization based collaborative filtering treats \n",
    "# the entries in the user-item matrix as explicit preferences given by the user to the item, \n",
    "# for example, users giving ratings to movies.\n",
    "# model = ALS.train(ratings, rank (Factors), numIterations, lambda)\n",
    "\n",
    "model=ALS.train(uaData,10,5,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Give below method a user id, and the number of recommendations we want\n",
    "recommendations=model.recommendProducts(user,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the row into a tuple of (Movie ID, Movie Name)\n",
    "\n",
    "moviesPath=\"C:/Users/emrijai/Documents/IPython Notebooks/MS7331/Project3/ml-100k/ml-100k/u.item\"\n",
    "moviesLookup=sc.textFile(moviesPath).map(lambda x:x.split(\"|\"))\n",
    "moviesLookup.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see which movies the user (specific user with userId) likes and rated 5\n",
    "\n",
    "userMovies=rawUserMovieData_wo_header\\\n",
    "    .map(lambda x:x.split(\",\"))\\\n",
    "    .filter(lambda x:int(x[1])==user and int(x[3])>4)\\\n",
    "    .map(lambda x:x[2]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the lookup action to print the names of the movies this user already likes\n",
    "for movies in userMovies: \n",
    "    print moviesLookup.lookup(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the user likes Action, War/Drama movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let’s print the recommended Artist names\n",
    "\n",
    "for rating in recommendations: \n",
    "    print moviesLookup.lookup(str(rating.product))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Latent Factor analysis and ALS are pretty magical. We just need to have a good dataset with User-Product Ratings\n",
    "The algorithm takes care of finding out the hidden factors that influence user’s preferences\n",
    "Running this in a spark cluster with millions or records will help us get better results quickly and with very less effort "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association rules from the Movielens dataset\n",
    "\n",
    "Association rules normally make sense with purchases / transactions datasets for example market basket analysis and hence stacking up specific products together logically. In this case, the rules we create may not make much sense, but they can help to determine and understand for example if a person who watches movie a will also be likely to have watched movie b, and hence we can see which movies are normally associate with each other with some minimum support and confidence. That way we can bucketize these movies together and display them on the screen of a specific user next to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The itertools module below will help us generate all permutations of movies\n",
    "We'll use that to find the possible rules and then filter for those with the required confidence\n",
    "\n",
    "Since this is a very expensive operation to iterate over all the permutations of such a huge dataset, we are going to increase the required support to 40% for this to work on a single laptop machine. We can use the logic on a large dataset using Spark Cluster Computing environment, where multiple worker nodes can work on the data set in parallel and generate the results much faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "allitems=[]\n",
    "\n",
    "def support(itemset):\n",
    "    userList=userItemRatingMatrix.index\n",
    "    nUsers=len(userList)\n",
    "    ratingMatrix=userItemRatingMatrix\n",
    "    for item in itemset:\n",
    "        ratingMatrix=ratingMatrix.loc[ratingMatrix.loc[:,item]>0]\n",
    "        #Subset the ratingMatrix to the set of users who have rated this item \n",
    "        userList=ratingMatrix.index\n",
    "    # After looping through all the items in the set, we are left only with the\n",
    "    # users who have rated all the items in the itemset\n",
    "    return float(len(userList))/float(nUsers)\n",
    "# Support is the proportion of all users who have watched this set of movies \n",
    "\n",
    "minsupport=0.4\n",
    "for item in list(userItemRatingMatrix.columns):\n",
    "    itemset=[item]\n",
    "    if support(itemset)>minsupport:\n",
    "        allitems.append(item)\n",
    "\n",
    "# We are now left only with the items which have been rated by atleast 40% of the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies were watched by atleast 40% of the users =  17\n",
      "\n",
      "From these movies we will generate rules and test again for support and confidence\n",
      "\n",
      "List of movie Ids [1, 7, 50, 56, 98, 100, 117, 121, 127, 174, 181, 237, 258, 286, 288, 294, 300]\n"
     ]
    }
   ],
   "source": [
    "print 'Number of movies were watched by atleast 40% of the users = ' , (len(allitems))\n",
    "\n",
    "print '\\nFrom these movies we will generate rules and test again for support and confidence'\n",
    "\n",
    "print '\\nList of movie Ids', allitems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below snippet will generate all possible 2 item rules which satisfy the support and confidence constraints. \n",
    "By Iterating over i, we can continue on  for finding 3 item rules or even n item rules. At each step make sure that every rule satisfies minconfidence and minsupport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minconfidence=0.2\n",
    "assocRules=[]\n",
    "i=2\n",
    "for rule in itertools.permutations(allitems,i):\n",
    "    #Generates all possible permutations of i items from the remaining list of movies \n",
    "    from_item=[rule[0]]\n",
    "    to_item=rule\n",
    "    # each rule is a tuple of i items \n",
    "    confidence=support(to_item)/support(from_item)\n",
    "    if confidence>minconfidence and support(to_item)>minsupport:\n",
    "        assocRules.append(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 50),\n",
       " (50, 1),\n",
       " (50, 100),\n",
       " (50, 174),\n",
       " (50, 181),\n",
       " (100, 50),\n",
       " (174, 50),\n",
       " (181, 50)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assocRules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
