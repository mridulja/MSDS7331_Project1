{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Lab\n",
    "\n",
    "## Brett Hallum, Mridul Jain, and Solomon Ndungu\n",
    "\n",
    "We start this lab by filtering out our unwanted data. A lot of this information was removed in Lab 1 as it did not make sense for us to use it due to missing values, single values for whole set, and a minimal distribution of values. On top of this removal, we do some imputation to fill in minor missing data values. Finally, we do some nominal classification on some of our data to order it for better use.\n",
    "\n",
    "We do create two additional data frames, loan_df_finished and loan_df_unfinished, at this point as well. Their purpose and usage will be discussed shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\emrijai\\\\Documents\\\\IPython Notebooks\\\\MS7331\\\\Project1\\\\MSDS7331_Project1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/emrijai/Documents/IPython Notebooks/MS7331/Project1/MSDS7331_Project1')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>installment</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>issue_d</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>...</th>\n",
       "      <th>Loan_Purpose_home_improvement</th>\n",
       "      <th>Loan_Purpose_house</th>\n",
       "      <th>Loan_Purpose_major_purchase</th>\n",
       "      <th>Loan_Purpose_medical</th>\n",
       "      <th>Loan_Purpose_moving</th>\n",
       "      <th>Loan_Purpose_other</th>\n",
       "      <th>Loan_Purpose_renewable_energy</th>\n",
       "      <th>Loan_Purpose_small_business</th>\n",
       "      <th>Loan_Purpose_vacation</th>\n",
       "      <th>Loan_Purpose_wedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>162.87</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>12/1/2011</td>\n",
       "      <td>27.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2500.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>59.83</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>12/1/2011</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>84.33</td>\n",
       "      <td>12252.0</td>\n",
       "      <td>12/1/2011</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>339.31</td>\n",
       "      <td>49200.0</td>\n",
       "      <td>12/1/2011</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>67.79</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>12/1/2011</td>\n",
       "      <td>17.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   funded_amnt  term  installment  annual_inc    issue_d    dti  delinq_2yrs  \\\n",
       "0       5000.0  36.0       162.87     24000.0  12/1/2011  27.65          0.0   \n",
       "1       2500.0  60.0        59.83     30000.0  12/1/2011   1.00          0.0   \n",
       "2       2400.0  36.0        84.33     12252.0  12/1/2011   8.72          0.0   \n",
       "3      10000.0  36.0       339.31     49200.0  12/1/2011  20.00          0.0   \n",
       "4       3000.0  60.0        67.79     80000.0  12/1/2011  17.94          0.0   \n",
       "\n",
       "   inq_last_6mths  open_acc  total_acc          ...           \\\n",
       "0             1.0       3.0        9.0          ...            \n",
       "1             5.0       3.0        4.0          ...            \n",
       "2             2.0       2.0       10.0          ...            \n",
       "3             1.0      10.0       37.0          ...            \n",
       "4             0.0      15.0       38.0          ...            \n",
       "\n",
       "   Loan_Purpose_home_improvement Loan_Purpose_house  \\\n",
       "0                            0.0                0.0   \n",
       "1                            0.0                0.0   \n",
       "2                            0.0                0.0   \n",
       "3                            0.0                0.0   \n",
       "4                            0.0                0.0   \n",
       "\n",
       "   Loan_Purpose_major_purchase  Loan_Purpose_medical  Loan_Purpose_moving  \\\n",
       "0                          0.0                   0.0                  0.0   \n",
       "1                          0.0                   0.0                  0.0   \n",
       "2                          0.0                   0.0                  0.0   \n",
       "3                          0.0                   0.0                  0.0   \n",
       "4                          0.0                   0.0                  0.0   \n",
       "\n",
       "   Loan_Purpose_other  Loan_Purpose_renewable_energy  \\\n",
       "0                 0.0                            0.0   \n",
       "1                 0.0                            0.0   \n",
       "2                 0.0                            0.0   \n",
       "3                 1.0                            0.0   \n",
       "4                 1.0                            0.0   \n",
       "\n",
       "   Loan_Purpose_small_business  Loan_Purpose_vacation  Loan_Purpose_wedding  \n",
       "0                          0.0                    0.0                   0.0  \n",
       "1                          0.0                    0.0                   0.0  \n",
       "2                          1.0                    0.0                   0.0  \n",
       "3                          0.0                    0.0                   0.0  \n",
       "4                          0.0                    0.0                   0.0  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "#Bring in data set\n",
    "loan_df = pd.read_csv('LoanData/loanData.csv')\n",
    "\n",
    "#Remove unwanted attributes from the data set based on specific criteria\n",
    "for col in ['id','funded_amnt_inv','grade','emp_title','pymnt_plan','url','title','zip_code','earliest_cr_line',\n",
    "            'mths_since_last_delinq','mths_since_last_record','pub_rec','revol_bal','revol_util',\n",
    "            'initial_list_status','out_prncp_inv','total_pymnt_inv','recoveries','collection_recovery_fee',\n",
    "            'last_pymnt_amnt','next_pymnt_d','last_credit_pull_d','collections_12_mths_ex_med','policy_code','application_type',\n",
    "            'annual_inc_joint','dti_joint','verification_status_joint','open_acc_6m','open_il_6m','open_il_12m','open_il_24m',\n",
    "            'mths_since_rcnt_il','total_bal_il','il_util','open_rv_12m','open_rv_24m','max_bal_bc','all_util',\n",
    "            'total_rev_hi_lim','inq_fi','total_cu_tl','inq_last_12m', 'mths_since_last_major_derog', 'desc', 'addr_state', 'tot_coll_amt', 'tot_cur_bal', 'member_id', 'total_rec_prncp', 'total_rec_int',\n",
    "           'total_rec_late_fee', 'loan_amnt', 'out_prncp']:\n",
    "    if col in loan_df:\n",
    "        del loan_df[col]\n",
    "\n",
    "#Create new variables for classification\n",
    "#Modify term to be integer for easier analysis\n",
    "loan_df = loan_df.replace(to_replace=\" 36 months\", value=36)\n",
    "loan_df = loan_df.replace(to_replace=\" 60 months\", value=60)\n",
    "\n",
    "#Impute missing data values with median\n",
    "for col in ['annual_inc', 'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'acc_now_delinq']:\n",
    "    df_temp = copy.deepcopy(loan_df[col])\n",
    "    df_temp = df_temp.replace(to_replace=np.nan, value=df_temp.median())\n",
    "    loan_df[col] = df_temp\n",
    "    \n",
    "#Numerical classification of grade\n",
    "tmp_df = pd.get_dummies(loan_df.sub_grade,prefix='sub_grade')\n",
    "loan_df = pd.concat((loan_df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "if \"sub_grade\" in loan_df:\n",
    "    del loan_df[\"sub_grade\"]\n",
    "    \n",
    "#Numerical classification of employment length\n",
    "tmp_df = pd.get_dummies(loan_df.emp_length,prefix='Employ')\n",
    "loan_df = pd.concat((loan_df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "if \"emp_length\" in loan_df:\n",
    "    del loan_df[\"emp_length\"]\n",
    "\n",
    "#Range of interest rates\n",
    "loan_df['Int_Rate_Range'] = pd.cut(loan_df.int_rate, [0, 5,10,15,20,25, 30], 6, labels=[0, 1, 2, 3, 4, 5])\n",
    "\n",
    "if \"int_rate\" in loan_df:\n",
    "    del loan_df[\"int_rate\"]\n",
    "\n",
    "#Nominal classification of loan_status\n",
    "loan_df.loc[loan_df['loan_status'] == 'Charged Off' , 'Loan_Class'] = 0\n",
    "loan_df.loc[loan_df['loan_status'] == 'Default' , 'Loan_Class'] = 0\n",
    "loan_df.loc[loan_df['loan_status'] == 'Late (31-120 days)' , 'Loan_Class'] = 0\n",
    "loan_df.loc[loan_df['loan_status'] == 'Late (16-30 days)' , 'Loan_Class'] = 0\n",
    "loan_df.loc[loan_df['loan_status'] == 'Does not meet the credit policy. Status:Charged Off' , 'Loan_Class'] = 0\n",
    "\n",
    "loan_df.loc[loan_df['loan_status'] == 'Fully Paid' , 'Loan_Class'] = 1\n",
    "loan_df.loc[loan_df['loan_status'] == 'Does not meet the credit policy. Status:Fully Paid' , 'Loan_Class'] = 1\n",
    "\n",
    "loan_df.loc[loan_df['loan_status'] == 'Current' , 'Loan_Class'] = 1\n",
    "loan_df.loc[loan_df['loan_status'] == 'In Grace Period' , 'Loan_Class'] = 1\n",
    "loan_df.loc[loan_df['loan_status'] == 'Issued' , 'Loan_Class'] = 1\n",
    "\n",
    "#Secondary classification of loan class for further analysis of data (used for new data frames)\n",
    "# comparing the historical data (good/bad loans) to current data (loans still in progress)\n",
    "# Loans are classified as 'Bad' if loans are bad\n",
    "loan_df.loc[loan_df['loan_status'] == 'Charged Off' , 'Loan_Class2'] = 'Bad'\n",
    "loan_df.loc[loan_df['loan_status'] == 'Default' , 'Loan_Class2'] = 'Bad'\n",
    "loan_df.loc[loan_df['loan_status'] == 'Late (31-120 days)' , 'Loan_Class2'] = 'Bad'\n",
    "loan_df.loc[loan_df['loan_status'] == 'Late (16-30 days)' , 'Loan_Class2'] = 'Bad'\n",
    "loan_df.loc[loan_df['loan_status'] == 'Does not meet the credit policy. Status:Charged Off' , 'Loan_Class2'] = 'Bad'\n",
    "# Loans are classified as 'Good' if loans are good\n",
    "loan_df.loc[loan_df['loan_status'] == 'Fully Paid' , 'Loan_Class2'] = 'Good'\n",
    "loan_df.loc[loan_df['loan_status'] == 'Does not meet the credit policy. Status:Fully Paid' , 'Loan_Class2'] = 'Good'\n",
    "# Loans are classified as 'Current' if loans are current/still in progress\n",
    "loan_df.loc[loan_df['loan_status'] == 'Current' , 'Loan_Class2'] = 'Current'\n",
    "loan_df.loc[loan_df['loan_status'] == 'In Grace Period' , 'Loan_Class2'] = 'Current'\n",
    "loan_df.loc[loan_df['loan_status'] == 'Issued' , 'Loan_Class2'] = 'Current'\n",
    "\n",
    "if \"loan_status\" in loan_df:\n",
    "    del loan_df[\"loan_status\"]\n",
    "\n",
    "#Nominal classification of verification status\n",
    "loan_df.loc[loan_df['verification_status'] == 'Verified', 'Verify_Status'] = 1\n",
    "loan_df.loc[loan_df['verification_status'] == 'Source Verified', 'Verify_Status'] = 1\n",
    "loan_df.loc[loan_df['verification_status'] == 'Not Verified', 'Verify_Status'] = 0\n",
    "\n",
    "if \"verification_status\" in loan_df:\n",
    "    del loan_df[\"verification_status\"]\n",
    "\n",
    "#Nominal classification of home ownership\n",
    "tmp_df = pd.get_dummies(loan_df.home_ownership,prefix='Home')\n",
    "loan_df = pd.concat((loan_df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "if \"home_ownership\" in loan_df:\n",
    "    del loan_df[\"home_ownership\"]\n",
    "\n",
    "#Impute missing data values with median\n",
    "for col in ['total_acc']:\n",
    "    df_temp = copy.deepcopy(loan_df[col])\n",
    "    df_temp = df_temp.replace(to_replace=np.nan, value= 0 )\n",
    "    loan_df[col] = df_temp    \n",
    "\n",
    "for col in ['total_pymnt']:\n",
    "    df_temp = copy.deepcopy(loan_df[col])\n",
    "    df_temp = df_temp.replace(to_replace=np.nan, value= 0 )\n",
    "    loan_df[col] = df_temp \n",
    "   \n",
    "\n",
    "tmp_df = pd.get_dummies(loan_df.purpose,prefix='Loan_Purpose')\n",
    "loan_df = pd.concat((loan_df,tmp_df),axis=1)\n",
    "\n",
    "if 'purpose' in loan_df:\n",
    "    del loan_df['purpose']\n",
    "    \n",
    "loan_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 119991 entries, 0 to 120127\n",
      "Data columns (total 83 columns):\n",
      "funded_amnt                        119991 non-null float64\n",
      "term                               119991 non-null float64\n",
      "installment                        119991 non-null float64\n",
      "annual_inc                         119991 non-null float64\n",
      "issue_d                            119991 non-null object\n",
      "dti                                119991 non-null float64\n",
      "delinq_2yrs                        119991 non-null float64\n",
      "inq_last_6mths                     119991 non-null float64\n",
      "open_acc                           119991 non-null float64\n",
      "total_acc                          119991 non-null float64\n",
      "total_pymnt                        119991 non-null float64\n",
      "last_pymnt_d                       119991 non-null object\n",
      "acc_now_delinq                     119991 non-null float64\n",
      "sub_grade_A1                       119991 non-null float64\n",
      "sub_grade_A2                       119991 non-null float64\n",
      "sub_grade_A3                       119991 non-null float64\n",
      "sub_grade_A4                       119991 non-null float64\n",
      "sub_grade_A5                       119991 non-null float64\n",
      "sub_grade_B1                       119991 non-null float64\n",
      "sub_grade_B2                       119991 non-null float64\n",
      "sub_grade_B3                       119991 non-null float64\n",
      "sub_grade_B4                       119991 non-null float64\n",
      "sub_grade_B5                       119991 non-null float64\n",
      "sub_grade_C1                       119991 non-null float64\n",
      "sub_grade_C2                       119991 non-null float64\n",
      "sub_grade_C3                       119991 non-null float64\n",
      "sub_grade_C4                       119991 non-null float64\n",
      "sub_grade_C5                       119991 non-null float64\n",
      "sub_grade_D1                       119991 non-null float64\n",
      "sub_grade_D2                       119991 non-null float64\n",
      "sub_grade_D3                       119991 non-null float64\n",
      "sub_grade_D4                       119991 non-null float64\n",
      "sub_grade_D5                       119991 non-null float64\n",
      "sub_grade_E1                       119991 non-null float64\n",
      "sub_grade_E2                       119991 non-null float64\n",
      "sub_grade_E3                       119991 non-null float64\n",
      "sub_grade_E4                       119991 non-null float64\n",
      "sub_grade_E5                       119991 non-null float64\n",
      "sub_grade_F1                       119991 non-null float64\n",
      "sub_grade_F2                       119991 non-null float64\n",
      "sub_grade_F3                       119991 non-null float64\n",
      "sub_grade_F4                       119991 non-null float64\n",
      "sub_grade_F5                       119991 non-null float64\n",
      "sub_grade_G1                       119991 non-null float64\n",
      "sub_grade_G2                       119991 non-null float64\n",
      "sub_grade_G3                       119991 non-null float64\n",
      "sub_grade_G4                       119991 non-null float64\n",
      "sub_grade_G5                       119991 non-null float64\n",
      "Employ_1 year                      119991 non-null float64\n",
      "Employ_10+ years                   119991 non-null float64\n",
      "Employ_2 years                     119991 non-null float64\n",
      "Employ_3 years                     119991 non-null float64\n",
      "Employ_4 years                     119991 non-null float64\n",
      "Employ_5 years                     119991 non-null float64\n",
      "Employ_6 years                     119991 non-null float64\n",
      "Employ_7 years                     119991 non-null float64\n",
      "Employ_8 years                     119991 non-null float64\n",
      "Employ_9 years                     119991 non-null float64\n",
      "Employ_< 1 year                    119991 non-null float64\n",
      "Employ_n/a                         119991 non-null float64\n",
      "Int_Rate_Range                     119991 non-null category\n",
      "Loan_Class                         119991 non-null float64\n",
      "Loan_Class2                        119991 non-null object\n",
      "Verify_Status                      119991 non-null float64\n",
      "Home_MORTGAGE                      119991 non-null float64\n",
      "Home_NONE                          119991 non-null float64\n",
      "Home_OTHER                         119991 non-null float64\n",
      "Home_OWN                           119991 non-null float64\n",
      "Home_RENT                          119991 non-null float64\n",
      "Loan_Purpose_car                   119991 non-null float64\n",
      "Loan_Purpose_credit_card           119991 non-null float64\n",
      "Loan_Purpose_debt_consolidation    119991 non-null float64\n",
      "Loan_Purpose_educational           119991 non-null float64\n",
      "Loan_Purpose_home_improvement      119991 non-null float64\n",
      "Loan_Purpose_house                 119991 non-null float64\n",
      "Loan_Purpose_major_purchase        119991 non-null float64\n",
      "Loan_Purpose_medical               119991 non-null float64\n",
      "Loan_Purpose_moving                119991 non-null float64\n",
      "Loan_Purpose_other                 119991 non-null float64\n",
      "Loan_Purpose_renewable_energy      119991 non-null float64\n",
      "Loan_Purpose_small_business        119991 non-null float64\n",
      "Loan_Purpose_vacation              119991 non-null float64\n",
      "Loan_Purpose_wedding               119991 non-null float64\n",
      "dtypes: category(1), float64(79), object(3)\n",
      "memory usage: 76.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "loan_df = loan_df.dropna()\n",
    "print loan_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "loan_df['duration_paid'] =  (pd.to_datetime(loan_df['last_pymnt_d']) - pd.to_datetime(loan_df['issue_d']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loan_df['Payment_rate'] =  (loan_df['total_pymnt'] / (loan_df['duration_paid'] / np.timedelta64(1, 'D')).astype(float) * 30) + 0.000001\n",
    "loan_df['Payment_ratio'] = loan_df['Payment_rate']/ loan_df['installment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "cannot convert float infinity to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d5420655c24f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloan_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mloan_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36mreplace\u001b[1;34m(self, to_replace, value, inplace, limit, regex, method, axis)\u001b[0m\n\u001b[0;32m   3394\u001b[0m                     new_data = self._data.replace(to_replace=to_replace,\n\u001b[0;32m   3395\u001b[0m                                                   \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3396\u001b[1;33m                                                   regex=regex)\n\u001b[0m\u001b[0;32m   3397\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mto_replace\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3398\u001b[0m                 if not (com.is_re_compilable(regex) or\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36mreplace\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   2877\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2878\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2879\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'replace'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2881\u001b[0m     def replace_list(self, src_list, dest_list, inplace=False, regex=False,\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mgr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2832\u001b[1;33m             \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2833\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36mreplace\u001b[1;34m(self, to_replace, value, inplace, filter, regex, convert, mgr)\u001b[0m\n\u001b[0;32m   1759\u001b[0m         return self._replace_single(to_replace, value, inplace=inplace,\n\u001b[0;32m   1760\u001b[0m                                     \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1761\u001b[1;33m                                     regex=regex, mgr=mgr)\n\u001b[0m\u001b[0;32m   1762\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1763\u001b[0m     def _replace_single(self, to_replace, value, inplace=False, filter=None,\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36m_replace_single\u001b[1;34m(self, to_replace, value, inplace, filter, regex, convert, mgr)\u001b[0m\n\u001b[0;32m   1797\u001b[0m                                                     \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1798\u001b[0m                                                     \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1799\u001b[1;33m                                                     mgr=mgr)\n\u001b[0m\u001b[0;32m   1800\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1801\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36mreplace\u001b[1;34m(self, to_replace, value, inplace, filter, regex, convert, mgr)\u001b[0m\n\u001b[0;32m    590\u001b[0m             values, _, to_replace, _ = self._try_coerce_args(self.values,\n\u001b[0;32m    591\u001b[0m                                                              to_replace)\n\u001b[1;32m--> 592\u001b[1;33m             \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask_missing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_replace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfilter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m                 \u001b[0mfiltered_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m~\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\pandas\\core\\common.pyc\u001b[0m in \u001b[0;36mmask_missing\u001b[1;34m(arr, values_to_mask)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m                 \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m             \u001b[1;31m# if x is a string and arr is not, then we get False and we must\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\pandas\\core\\categorical.pyc\u001b[0m in \u001b[0;36mf\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mother\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m                 \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_codes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\pandas\\indexes\\base.pyc\u001b[0m in \u001b[0;36m__contains__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1227\u001b[0m         \u001b[1;31m# work around some kind of odd cython bug\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1228\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1229\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1230\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1231\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.__contains__ (pandas\\index.c:2824)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.__contains__ (pandas\\hashtable.c:6304)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: cannot convert float infinity to integer"
     ]
    }
   ],
   "source": [
    "loan_df.replace([np.inf, -np.inf], np.nan)\n",
    "print loan_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loan_df = loan_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create separate data frames for future analysis and breakdown\n",
    "loan_df_unfinished = loan_df[loan_df['Loan_Class2'] == 'Current'] #Data of uncompleted loans\n",
    "loan_df_finished = loan_df[(loan_df['Loan_Class2'] == 'Bad') | (loan_df['Loan_Class2']=='Good')] #Data of completed loans\n",
    "\n",
    "\n",
    "loan_df.copy()\n",
    "\n",
    "if \"Loan_Class2\" in loan_df:\n",
    "    del loan_df[\"Loan_Class2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if \"issue_d\" in loan_df:\n",
    "    del loan_df[\"issue_d\"]\n",
    "if \"last_payment_d\" in loan_df:\n",
    "    del loan_df[\"last_pymnt_d\"]\n",
    "if \"total_payment\" in loan_df:\n",
    "    del loan_df[\"total_pymnt\"]\n",
    "if \"duration_paid\" in loan_df:\n",
    "    del loan_df[\"duration_paid\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loan_df['Payment_rate'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Split our data frame into our classification of Loan_Class and the rest of the data set\n",
    "if 'Loan_Class' in loan_df:\n",
    "    y = loan_df['Loan_Class'].values\n",
    "    del loan_df['Loan_Class']\n",
    "    X = loan_df.values\n",
    "\n",
    "# Shuffle split our data into an 80/20 breakdown. The 80 will be used for training and the 20 for testing\n",
    "num_cv_iterations = 5\n",
    "num_instances = len(y)\n",
    "cv_obj = ShuffleSplit(n = num_instances, n_iter = num_cv_iterations, test_size = 0.2)\n",
    "\n",
    "print cv_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "We will begin by conducting a logistic regression on the full data set. We will look at the accuracy of the classification to see how accurate the regression model is and the confusion matrix to see if there are any oddities in the breakdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "logreg_obj = LogisticRegression(penalty='l2', C=1.0, class_weight=None)\n",
    "\n",
    "# Fit and test the regression model for each shuffled instance\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_obj):\n",
    "    logreg_obj.fit(X[train_indices],y[train_indices])  # train object\n",
    "    y_hat = logreg_obj.predict(X[test_indices]) # get test set precitions\n",
    "\n",
    "    # print the accuracy and confusion matrix \n",
    "    print \"====Iteration\",iter_num,\" ====\"\n",
    "    print \"accuracy\", mt.accuracy_score(y[test_indices],y_hat) \n",
    "    print \"confusion matrix\\n\",mt.confusion_matrix(y[test_indices],y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add column to determine if loan is good or bad based on classification in data set\n",
    "loan_df_finished['isGoodLoan'] = loan_df_finished['Loan_Class2'] == 'Good'\n",
    "loan_df_finished.isGoodLoan = loan_df_finished.isGoodLoan.astype(np.int)\n",
    "\n",
    "#Remove Loan_Class and Loan_Class2 from the data frame so they are no longer observed for classification\n",
    "if \"Loan_Class\" in loan_df_finished:\n",
    "    del loan_df_finished[\"Loan_Class\"]\n",
    "if \"Loan_Class2\" in loan_df_finished:\n",
    "    del loan_df_finished[\"Loan_Class2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into our x and y matrices for testing\n",
    "if 'isGoodLoan' in loan_df_finished:\n",
    "    updated_y = loan_df_finished['isGoodLoan'].values\n",
    "    del loan_df_finished['isGoodLoan']\n",
    "    updated_X = loan_df_finished.values\n",
    "\n",
    "# Create a new Shuffle split on the narrowed classification data\n",
    "num_cv_iterations = 5\n",
    "num_instances = len(updated_y)\n",
    "updated_cv_obj = ShuffleSplit(n = num_instances, n_iter = num_cv_iterations, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updated_logreg_obj = LogisticRegression(penalty='l2', C=10, class_weight=None)\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(updated_cv_obj):\n",
    "    updated_logreg_obj.fit(updated_X[train_indices],updated_y[train_indices])  # train object\n",
    "    updated_y_hat = updated_logreg_obj.predict(updated_X[test_indices]) # get test set precitions\n",
    "\n",
    "    # print the accuracy and confusion matrix \n",
    "    print \"==== Iteration\",iter_num,\" ====\"\n",
    "    print \"accuracy\", mt.accuracy_score(updated_y[test_indices],updated_y_hat) \n",
    "    print \"confusion matrix\\n\",mt.confusion_matrix(updated_y[test_indices],updated_y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this classification not only have a higher accuracy than the original, full data set, right around 97.9% on average rather than 93.1%, but it also has better looking confusion mattrices. There are data points that are classified correctly. Although there are still a large number of bad loans being incorrectly identified as good, it is much better distributed than the original set.\n",
    "\n",
    "We follow up this analysis by looking at the weight of each factor used. This will help us figure out if there are certain features that are more influential than others to help with the classification of each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Non-normalized weights\n",
    "# Iterate over the coefficients to get the weight for each variable\n",
    "weights = updated_logreg_obj.coef_.T # take transpose to make a column vector\n",
    "variable_names = loan_df_finished.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print name, 'has weight of', coef[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to normalize the weights for each variable so that a much larger valued variable, such as the funded amount, is weighted the same as a small valued variable such as one that is binary or nominal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Normalize the features\n",
    "scale_obj = StandardScaler()\n",
    "scale_obj.fit(updated_X[train_indices])\n",
    "\n",
    "X_train_scaled = scale_obj.transform(updated_X[train_indices]) # apply to training\n",
    "X_test_scaled = scale_obj.transform(updated_X[test_indices])\n",
    "\n",
    "# Fit the regression to the new scaled data\n",
    "updated_logreg_obj.fit(X_train_scaled, updated_y[train_indices])\n",
    "\n",
    "y_hat = updated_logreg_obj.predict(X_test_scaled)\n",
    "\n",
    "# Rescore the accuracy and confusion matrix for the scaled data\n",
    "acc = mt.accuracy_score(updated_y[test_indices],y_hat)\n",
    "conf = mt.confusion_matrix(updated_y[test_indices],y_hat)\n",
    "print 'accuracy:', acc \n",
    "print conf\n",
    "\n",
    "# Sort the attributes on size and print them out in order of weight\n",
    "zip_vars = zip(updated_logreg_obj.coef_.T,loan_df_finished.columns) # combine attributes\n",
    "zip_vars.sort(key = lambda t: np.abs(t[0])) # sort them by the magnitude of the weight\n",
    "for coef, name in zip_vars:\n",
    "    print name, 'has weight of', coef[0] # now print them out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scaling the data, we still get a similar accuracy to before and our confusion matrix has remained proportional. The scaled view of the variables provides a better way to compare each of the variables to one another and see which have a significant impact. In the graph below, it is even easier to note which variables have the most impact. The funded amount and total payment variables have a significant impact over any of the rest of the variables. Other notable variables that are valued slightly higher than the other factors are the Interst Rate Range, annual income and the number of total accounts someone has.\n",
    "\n",
    "These values make a lot of sense as to why they are the most important. The total payment should definitely be a factor that plays into determining if a loan is good or bad. If the loan is in good standing, the full principle amount will be paid off. This will relate positively because as you pay more back to the bank, you will become closer to finishing the loan. Alternatively, the less money you have paid the more likely you are to default on the loan. Funded amount and interest rate range are also valuable. Banks probably use these in relation to one another to determine who will default and give certain rates for ceratin funded amounts. They are negatively weighted in this case which means the higher the interest rate and the higher the funded amount, the more likely someone is to default. As a loan increases in value and interest rate, it gets more difficult to pay off the loan and so someone is more likely to go into default and end up with a \"bad\" loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "weights = pd.Series(updated_logreg_obj.coef_[0],index=loan_df_finished.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "We continue our trials by observing the Support Vector Machines (SVM) to see if this method of classification can classify our data set better than logistic regression. We will start by observing the SVM of our original data set. Although it will most likely turn out the same way, not giving very good data, the SVM may reveal new splits in the decision. We will follow that up with looking at the SVM of the reduced data set that contains the completed loans. We can compare the accuracy of the SVM with the above logistic regression to determine if one classifies this data better than the other.\n",
    "\n",
    "Note: We use the SGDClassifier due to the size of our data set. Both sets are large and would take too long to train with a normal SVM in a time that is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVM of full data set\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# SGDClassifier using 'hinge' for loss to get an SVM\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Get SVM classifier\n",
    "regularize_const = 0.1\n",
    "iterations = 5\n",
    "svm_sgd = SGDClassifier(alpha=regularize_const,\n",
    "        fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
    "        loss='hinge', n_iter=iterations, n_jobs=-1, penalty='l2')\n",
    "\n",
    "# Fit our data (after it is scaled) to an SVM classification\n",
    "scl = StandardScaler()\n",
    "for train_idx, test_idx in cv_obj:\n",
    "    svm_sgd.fit(scl.fit_transform(X[train_idx]),y[train_idx])\n",
    "    yhat = svm_sgd.predict(scl.transform(X[test_idx]))\n",
    "    \n",
    "    conf = mt.confusion_matrix(y[test_idx],yhat)\n",
    "    acc = mt.accuracy_score(y[test_idx],yhat)\n",
    "\n",
    "print 'SVM:', acc\n",
    "print 'Confusion Matrix:\\n', conf\n",
    "print 'Coefficients', svm_sgd.coef_\n",
    "weights = pd.Series(svm_sgd.coef_[0],index=loan_df_finished.columns)\n",
    "weights.plot(kind='bar', figsize=(12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the full data set remained very similar. It stayed at around 93% accuracy, but still has the same style of confusion matrix from the logistic regression with bad loans being classified as good ones. We now look at the SVM of the reduced data to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVM of completed loan data set\n",
    "# SGDClassifier using 'hinge' for loss to get an SVM\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "regularize_const = 0.1\n",
    "iterations = 5\n",
    "svm_sgd = SGDClassifier(alpha=regularize_const,\n",
    "        fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
    "        loss='hinge', n_iter=iterations, n_jobs=-1, penalty='l2')\n",
    "\n",
    "scl = StandardScaler()\n",
    "for train_idx, test_idx in updated_cv_obj:\n",
    "    svm_sgd.fit(scl.fit_transform(updated_X[train_idx]),updated_y[train_idx])\n",
    "    yhat = svm_sgd.predict(scl.transform(updated_X[test_idx]))\n",
    "    \n",
    "    conf = mt.confusion_matrix(updated_y[test_idx],yhat)\n",
    "    acc = mt.accuracy_score(updated_y[test_idx],yhat)\n",
    "\n",
    "print 'SVM:', acc\n",
    "print 'Confusion Matrix:\\n', conf\n",
    "print 'Coefficients', svm_sgd.coef_\n",
    "weights = pd.Series(svm_sgd.coef_[0],index=loan_df_finished.columns)\n",
    "weights.plot(kind='bar', figsize=(12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM for the reduced data, which includes just completed loans, has a much lower accuracy than the logistic regression classification of the same data. The accuracy dropped from just below 98.0% to around 86.8%. This is a significant difference. It is possible that part of this is due to SGDClassifier taking the Stochastic Gradient instead of the full gradient of the SVM. Although the SGD runs faster than the linear regression model we used (LinearRegression()), it does not have the accuracy that we would expect or want in this situation, especially when the original data stayed around the same accuracy between the two different classifications.\n",
    "\n",
    "It is interesting to note that the classes that the SVM pulls out are almost identical to the ones the logistic regression uses. Funded amount and total payment are still at the top of the list with Interest Rate Range being a distant fourth, but still near the top. Surprisingly, it has added importance on the 'Term' variable that the logistic regression did not pull out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Analysis: Are there better variable choices?\n",
    "\n",
    "In this section, we will use the logistic regression classification, which seemed to perform better for our reduced data set, to see if we can increase the accuracy of our model by looking at different variable inputs. The idea is to reduce the number of inputs to try and improve the data. What if we look at our major regression factors and limit our dataset to those factors? Do we improve accuracy any by limiting the data given to the classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "used_temp_df = pd.DataFrame()\n",
    "major_factors_df = loan_df_finished\n",
    "\n",
    "for col in ['funded_amnt', 'Int_Rate_Range', 'annual_inc', 'total_pymnt', 'Verify_Status']:\n",
    "    if col in major_factors_df:\n",
    "        used_temp_df[col] = major_factors_df[col]\n",
    "        \n",
    "updated_X = used_temp_df.values\n",
    "\n",
    "updated_logreg_obj = LogisticRegression(penalty='l2', C=1.0, class_weight=None)\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(updated_cv_obj):\n",
    "    updated_logreg_obj.fit(updated_X[train_indices],updated_y[train_indices])  # train object\n",
    "    updated_y_hat = updated_logreg_obj.predict(updated_X[test_indices]) # get test set precitions\n",
    "\n",
    "    # print the accuracy and confusion matrix \n",
    "    print \"====Iteration\",iter_num,\" ====\"\n",
    "    print \"accuracy\", mt.accuracy_score(updated_y[test_indices],updated_y_hat) \n",
    "    print \"confusion matrix\\n\",mt.confusion_matrix(updated_y[test_indices],updated_y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scl = StandardScaler()\n",
    "scl.fit(updated_X[train_indices])\n",
    "\n",
    "X_train_scaled = scl.transform(updated_X[train_indices]) # apply to training\n",
    "X_test_scaled = scl.transform(updated_X[test_indices])\n",
    "\n",
    "# Fit the regression to the new scaled data\n",
    "updated_logreg_obj.fit(X_train_scaled, updated_y[train_indices])\n",
    "\n",
    "y_hat = updated_logreg_obj.predict(X_test_scaled)\n",
    "\n",
    "# Rescore the accuracy and confusion matrix for the scaled data\n",
    "acc = mt.accuracy_score(updated_y[test_indices],y_hat)\n",
    "conf = mt.confusion_matrix(updated_y[test_indices],y_hat)\n",
    "print 'accuracy:', acc \n",
    "print conf\n",
    "weights = pd.Series(updated_logreg_obj.coef_[0],index=used_temp_df.columns)\n",
    "weights.plot(kind='bar', figsize=(12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The unscaled data of this significantly reduced data set has a marginally better fit than the original reduced data set. If you compare the 98.1% average of the newly reduced data set to the 97.9% of the base reduced data, you get a slightly better performance by taking out some of the less important, according to the logistic regression, variables. The scaled data comes in at a similar accuracy rating of the original sample at 97.9% so there really is not a huge change here. Even looking at the weights of each, the funded amount and total payment factors still far outweigh the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Payment_Ratio = loan_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
